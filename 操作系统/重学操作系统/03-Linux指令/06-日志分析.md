##11|高级技巧之日志分析：利用Linux指令分析Web日志

著名的黑客、自由软件运动的先驱理查德.斯托曼说过，“编程不是科学，编程是手艺”。可见，要想真正搞好编程，除了学习理论知识，还需要在实际的工作场景中进行反复的锤炼。

所以今天我们将结合实际的工作场景，带你利用Linux指令分析Web日志，这其中包含很多小技巧，掌握了本课时的内容，将对你将来分析线上日志、了解用户行为和查找问题有非常大地帮助。

本课时将用到一个大概有5W多条记录的nginx日志文件，你可以在GitHub上下载。下面就请你和我一起，通过分析这个nginx日志文件，去锤炼我们的手艺。

###第一步：能不能这样做？

当我们想要分析一个线上文件的时候，首先要思考，能不能这样做？这里你可以先用htop指令看一下当前的负载。如果你的机器上没有htop，可以考虑用yum或者apt去安装。

![](/Users/weihuchao/Pictures/src/20211203/重学操作系统/CgqCHl-BkJ6AcP32AAduMy8fcSw412.png)

如上图所示，我的机器上8个CPU都是0负载，2G的内存用了一半多，还有富余。我们用wget将目标文件下载到本地（如果你没有wget，可以用yum或者apt安装）。

```
wget某网址（自己替代）
```

然后我们用ls查看文件大小。发现这只是一个7M的文件，因此对线上的影响可以忽略不计。如果文件太大，建议你用scp指令将文件拷贝到闲置服务器再分析。下图中我使用了`--block-size`让ls以M为单位显示文件大小。

![](/Users/weihuchao/Pictures/src/20211203/重学操作系统/Ciqc1F-BkKeAQDs9AACqJbZ2jCM025.png)

确定了当前机器的CPU和内存允许我进行分析后，我们就可以开始第二步操作了。

###第二步：LESS日志文件

在分析日志前，给你提个醒，记得要less一下，看看日志里面的内容。之前我们说过，尽量使用less这种不需要读取全部文件的指令，因为在线上执行cat是一件非常危险的事情，这可能导致线上服务器资源不足。

![](/Users/weihuchao/Pictures/src/20211203/重学操作系统/CgqCHl-BkK6AcDGvAAjaPXe-Nbc605.png)

如上图所示，我们看到nginx的access_log每一行都是一次用户的访问，从左到右依次是：

*IP地址；

*时间；

*HTTP请求的方法、路径和协议版本、返回的状态码；

*UserAgent。

###第三步：PV分析

PV（PageView），用户每访问一个页面就是一次PageView。对于nginx的acess_log来说，分析PV非常简单，我们直接使用wc-l就可以看到整体的PV。

![](/Users/weihuchao/Pictures/src/20211203/重学操作系统/Ciqc1F-BkL6AGiY-AABQPMnGu40979.png)

如上图所示：我们看到了一共有51462条PV。

####第四步：PV分组

通常一个日志中可能有几天的PV，为了得到更加直观的数据，有时候需要按天进行分组。为了简化这个问题，我们先来看看日志中都有哪些天的日志。

使用`awk'{print$4}'access.log|less`可以看到如下结果。awk是一个处理文本的领域专有语言。这里就牵扯到**领域专有语言**这个概念，英文是**DomainSpecificLanguage**。领域专有语言，就是为了处理某个领域专门设计的语言。比如awk是用来分析处理文本的DSL，html是专门用来描述网页的DSL，SQL是专门用来查询数据的DSL……大家还可以根据自己的业务设计某种针对业务的DSL。

你可以看到我们用$4代表文本的第4列，也就是时间所在的这一列，如下图所示：

![](/Users/weihuchao/Pictures/src/20211203/重学操作系统/CgqCHl-BkMaAb421AAGUr-N08hM187.png)

我们想要按天统计，可以利用awk提供的字符串截取的能力。

![](/Users/weihuchao/Pictures/src/20211203/重学操作系统/CgqCHl-BkMuAKo9UAAIcPR902XQ858.png)

上图中，我们使用awk的substr函数，数字2代表从第2个字符开始，数字11代表截取11个字符。

接下来我们就可以分组统计每天的日志条数了。

![](/Users/weihuchao/Pictures/src/20211203/重学操作系统/CgqCHl-BkNGAB-VgAASNmct9nQA628.png)

上图中，使用`sort`进行排序，然后使用`uniq-c`进行统计。你可以看到从2015年5月17号一直到6月4号的日志，还可以看到每天的PV量大概是在2000~3000之间。

###第五步：分析UV

接下来我们分析UV。UV（UniqVisitor），也就是统计访问人数。通常确定用户的身份是一个复杂的事情，但是我们可以用IP访问来近似统计UV。

![](/Users/weihuchao/Pictures/src/20211203/重学操作系统/Ciqc1F-BkNeAam2YAACxCjlKsvc488.png)

上图中，我们使用awk去打印$1也就是第一列，接着sort排序，然后用uniq去重，最后用wc-l查看条数。这样我们就知道日志文件中一共有2660个IP，也就是2660个UV。

###第六步：分组分析UV

接下来我们尝试按天分组分析每天的UV情况。这个情况比较复杂，需要较多的指令，我们先创建一个叫作`sum.sh`的`bash`脚本文件，写入如下内容：

```shell
#!/usr/bin/bash
awk'{printsubstr($4,2,11)""$1}'access.log|\
	sort|uniq|\
	awk'{uv[$1]++;next}END{for(ipinuv)printip,uv[ip]}'
```

具体分析如下。

*文件首部我们使用`#!`，表示我们将使用后面的`/usr/bin/bash`执行这个文件。

*第一次awk我们将第4列的日期和第1列的ip地址拼接在一起。

*下面的sort是把整个文件进行一次字典序排序，相当于先根据日期排序，再根据IP排序。

*接下来我们用uniq去重，日期+IP相同的行就只保留一个。

*最后的awk我们再根据第1列的时间和第2列的IP进行统计。

为了理解最后这一行描述，我们先来简单了解下awk的原理。

awk本身是逐行进行处理的。因此我们的next关键字是提醒awk跳转到下一行输入。对每一行输入，awk会根据第1列的字符串（也就是日期）进行累加。之后的END关键字代表一个触发器，就是END后面用{}括起来的语句会在所有输入都处理完之后执行——当所有输入都执行完，结果被累加到uv中后，通过foreach遍历uv中所有的key，去打印ip和ip对应的数量。

编写完上面的脚本之后，我们保存退出编辑器。接着执行chmod+x./sum.sh，给sum.sh增加执行权限。然后我们可以像下图这样执行，获得结果：

![](/Users/weihuchao/Pictures/src/20211203/重学操作系统/CgqCHl-BkOKAfpNwAAOFk0EhDjU183.png)

如上图，IP地址已经按天进行统计好了。

###总结

今天我们结合一个简单的实战场景——Web日志分析与统计练习了之前学过的指令，提高熟练程度。此外，我们还一起学习了新知识——功能强大的awk文本处理语言。在实战中，我们对一个nginx的access_log进行了简单的数据分析，直观地获得了这个网站的访问情况。

我们在日常的工作中会遇到各种各样的日志，除了nginx的日志，还有应用日志、前端日志、监控日志等等。你都可以利用今天学习的方法，去做数据分析，然后从中得出结论。

####思考题

接下来我给你出2个场景思考题，帮助你继续练习使用Linux指令。

根据今天的access_log分析出有哪些终端访问了这个网站，并给出分组统计结果。

根据今天的access_log分析出访问量Top前三的网页。
